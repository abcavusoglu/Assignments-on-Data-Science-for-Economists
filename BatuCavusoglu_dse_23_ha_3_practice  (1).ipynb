{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# DSE HA-3\n",
        "# **Creating your own Sorting Hogwarts Hat âœ¨**"
      ],
      "metadata": {
        "id": "M4TC5R4WaZoI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The aim of this task is to constuct ML model that predicts  Hogwarts Houses based on students forces (the target variable is `Hogwarts House`)\n",
        "\n",
        "Good luck, dear wizards!"
      ],
      "metadata": {
        "id": "HCxHikKAnH0P"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J-9Xfjg3X5-V"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "data = pd.read_csv(\"https://raw.githubusercontent.com/Shuaynat/DSE-23-24/main/02-home-assignments/ha-03-practice/ha-3_dataset.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data.head()"
      ],
      "metadata": {
        "id": "sjMne2P2sKhH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1 Data preparation"
      ],
      "metadata": {
        "id": "QS3wzfS6eMo2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Follow several preparation steps:\n",
        "\n",
        "1. Drop columns that we will not use for prediction: last name, first name and index.\n",
        "2. Have a look at the correlation plot. If you see that some features are too correlated - drop some of them\n",
        "3. Fill mising features with mean taret encoding\n",
        "4. Convert categorical variables `Best Hand` and `Hogwart House` to numerical.\n",
        "5. Create from `Birthday` new fearures `day`, `month`, `year`\n",
        "6. Standardize features\n",
        "7. Separate data into test and train sets (leave 33% of data in test set)\n"
      ],
      "metadata": {
        "id": "a5tSzltofoaV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(data.columns)\n",
        "data.info()"
      ],
      "metadata": {
        "id": "gbwwA7ILdoYB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1-dropping columns\n",
        "data.drop(columns=['Index','First Name','Last Name'],inplace=True)"
      ],
      "metadata": {
        "id": "uixLrGhjuACh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2-correlation\n",
        "\n",
        "import numpy as np\n",
        "corr_matrix = data.corr().abs()\n",
        "\n",
        "# Select upper triangle of correlation matrix\n",
        "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
        "\n",
        "# correlation greater than 0.8\n",
        "dropping_criteria = [column for column in upper.columns if any(upper[column] > 0.8)]\n",
        "\n",
        "# Drop the features\n",
        "data.drop(dropping_criteria, axis=1, inplace=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-d-fQeVGp6dY",
        "outputId": "bd8bd919-96c1-46fc-9a56-b76c855a11d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-60d30d7c37d5>:4: FutureWarning: The default value of numeric_only in DataFrame.corr is deprecated. In a future version, it will default to False. Select only valid columns or specify the value of numeric_only to silence this warning.\n",
            "  corr_matrix = data.corr().abs()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3-Filling missing features mean\n",
        "import numpy as np\n",
        "\n",
        "# Finding the mean of the column that have NaN values and replacing with the mean\n",
        "mean_value = data['Arithmancy'].mean()\n",
        "data['Arithmancy'].fillna(value=mean_value, inplace=True)\n",
        "mean_value = data['Astronomy'].mean()\n",
        "data['Astronomy'].fillna(value=mean_value, inplace=True)\n",
        "mean_value = data['Herbology'].mean()\n",
        "data['Herbology'].fillna(value=mean_value, inplace=True)\n",
        "mean_value = data['Divination'].mean()\n",
        "data['Divination'].fillna(value=mean_value, inplace=True)\n",
        "mean_value = data['Muggle Studies'].mean()\n",
        "data['Muggle Studies'].fillna(value=mean_value, inplace=True)\n",
        "mean_value = data['Ancient Runes'].mean()\n",
        "data['Ancient Runes'].fillna(value=mean_value, inplace=True)\n",
        "mean_value = data['History of Magic'].mean()\n",
        "data['History of Magic'].fillna(value=mean_value, inplace=True)\n",
        "mean_value = data['Potions'].mean()\n",
        "data['Potions'].fillna(value=mean_value, inplace=True)\n",
        "mean_value = data['Care of Magical Creatures'].mean()\n",
        "data['Care of Magical Creatures'].fillna(value=mean_value, inplace=True)"
      ],
      "metadata": {
        "id": "wfo8qp4JgpAE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4-Converting categorical variables\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "le = LabelEncoder()\n",
        "data['Best Hand'] = le.fit_transform(data['Best Hand'])\n",
        "data['Hogwarts House'] = le.fit_transform(data['Hogwarts House'])"
      ],
      "metadata": {
        "id": "786Hrf7Ajsxo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5-from Birthday new fearures day, month, year\n",
        "import datetime\n",
        "import pandas as pd\n",
        "data[\"Birthday\"] = pd.to_datetime(data[\"Birthday\"])\n",
        "\n",
        "data['Day']=data[\"Birthday\"].dt.day\n",
        "data['Month']=data[\"Birthday\"].dt.month\n",
        "data['Year']=data[\"Birthday\"].dt.year\n",
        "\n",
        "data.drop(columns=['Birthday'],inplace=True)"
      ],
      "metadata": {
        "id": "Z1-7kCtGl1SM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_ = data['Hogwarts House']\n",
        "X_ = data.drop(['Hogwarts House'], axis=1)"
      ],
      "metadata": {
        "id": "U0YrGv3hLyAX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "# Initialize a scaler\n",
        "scaler = StandardScaler()\n",
        "# Fit and transform the data\n",
        "X = scaler.fit_transform(X_)\n",
        "y = y_"
      ],
      "metadata": {
        "id": "LKVqv0hbKcd3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 7-Separate data into test and train sets (leave 33% of data in test set)\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42,train_size = 0.66)"
      ],
      "metadata": {
        "id": "xJI44bPH1vPv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Multiclass logit model with regularization\n"
      ],
      "metadata": {
        "id": "Wg4A3zLH3VaR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Estimate multiclass logit model:\n",
        "\n",
        "1. Use sklearn realization of logistic regression in [SGDC classifier](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html)\n",
        "\n",
        "\n",
        "2. Use cross-validation to choose type of regularization (penalty equal to `l1`, `l2`, or `elastic net`) and regularization weight (`alpha` in  `range(start=0.1, 1, step=0.1)`)\n"
      ],
      "metadata": {
        "id": "1iA1U9tNBjCe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, x_train, y_train, x_test, y_test):\n",
        "    train_preds = model.predict(x_train)\n",
        "    test_preds = model.predict(x_test)\n",
        "    train_acc = metrics.accuracy_score(y_train, train_preds)\n",
        "    test_acc = metrics.accuracy_score(y_test, test_preds)\n",
        "    print('Train accuracy: %s' % train_acc)\n",
        "    print('Test accuracy: %s' % test_acc)"
      ],
      "metadata": {
        "id": "m8NtGFs3cplo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import linear_model, metrics, model_selection\n",
        "import numpy as np\n",
        "grid_search = model_selection.GridSearchCV(\n",
        "    estimator=linear_model.SGDClassifier(),\n",
        "    param_grid={'alpha': np.arange(0.1, 1, 0.1),\n",
        "                'penalty':['l2','l1','elasticnet']},\n",
        "    cv=10,\n",
        "    return_train_score=True)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "r = pd.DataFrame(grid_search.cv_results_)\n",
        "print(r[['params', 'mean_train_score', 'mean_test_score']])\n",
        "\n",
        "best_alpha = grid_search.best_params_['alpha']\n",
        "best_penalty = grid_search.best_params_['penalty']\n",
        "print('Best alpha: %s' % best_alpha)\n",
        "print('Best penalty: %s' % best_penalty)"
      ],
      "metadata": {
        "id": "l-knO7pebI1K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_model = linear_model.SGDClassifier(alpha=best_alpha, penalty=best_penalty)\n",
        "best_model.fit(X_train, y_train)\n",
        "evaluate(best_model, X_train, y_train, X_test, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v3l-ntYacauD",
        "outputId": "ec2fc114-5963-4f09-99e4-d57a273beb76"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train accuracy: 0.9801136363636364\n",
            "Test accuracy: 0.9852941176470589\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "# Always scale the input. The most convenient way is to use a pipeline.\n",
        "logit_model = SGDClassifier(penalty='l2',alpha=0.1)\n",
        "logit_model.fit(X_train, y_train)\n",
        "\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "y_predict_logit_model = logit_model.predict(X_test)"
      ],
      "metadata": {
        "id": "UVQRY0wJ-6dm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Calculate average marginal effects\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "-OL-c93Z-9Nz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For each of the variables calculate average marginal effect on the probability."
      ],
      "metadata": {
        "id": "kwySpvMpM8JH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def logit_margeff(model, X, features, kind='probability'):\n",
        "\n",
        "    coef = model.coef_\n",
        "    intercept = model.intercept_\n",
        "\n",
        "    if kind == 'probability':\n",
        "\n",
        "        logodds = intercept + X @ coef.T\n",
        "\n",
        "        marg_effects=[]\n",
        "        for i in range(coef.size):\n",
        "            marg_eff = np.mean(coef[0,i]*np.exp(-logodds)/(1+np.exp(-logodds))**2).round(3)[0]\n",
        "            marg_effects.append(marg_eff)\n",
        "\n",
        "\n",
        "    elif kind == \"odds\":\n",
        "\n",
        "        marg_effects=[]\n",
        "        for i in range(coef.size):\n",
        "            marg_eff = (np.exp(coef[0,i])).round(3)\n",
        "            marg_effects.append(marg_eff)\n",
        "\n",
        "    marginal_effects = {}\n",
        "    marginal_effects['features'] = features\n",
        "    marginal_effects[f'marginal_effects_{kind}'] = marg_effects\n",
        "\n",
        "    df = pd.DataFrame(marginal_effects)\n",
        "\n",
        "    return df"
      ],
      "metadata": {
        "id": "TBtwMO1TkOMQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features = list(data.iloc[:,1:13].columns)\n",
        "print(features)\n",
        "\n",
        "logit_model = SGDClassifier(penalty='l2',alpha=0.1)\n",
        "logit_model.fit(data[features], data['Hogwarts House'])\n",
        "\n",
        "\n",
        "logit_margeff(logit_model,data[features],features,kind='probability')\n"
      ],
      "metadata": {
        "id": "xHPnL6NNKBYJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Build logit with PCA transformation"
      ],
      "metadata": {
        "id": "ssl8BGkmMi4q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Extract 5 principal components and evaluate logit model using them as explanatory variables"
      ],
      "metadata": {
        "id": "p1TM1QmeMyvz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# YOUR CODE HERE\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "pca = PCA(n_components=5).fit(X_train)\n",
        "X_train = pca.transform(X_train)\n",
        "X_test = pca.transform(X_test)\n",
        "\n",
        "# Always scale the input. The most convenient way is to use a pipeline.\n",
        "logitpca_model = SGDClassifier(penalty='l2',alpha=0.1)\n",
        "logitpca_model.fit(X_train, y_train)\n",
        "\n",
        "y_predict_logitpca_model = logitpca_model.predict(X_test)"
      ],
      "metadata": {
        "id": "mDgslv8GM5ZU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Build classification forest\n"
      ],
      "metadata": {
        "id": "lwGflYf2N-2p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You are free to add PCA-transformed explanatory variables as predictors. Tune parameters by cross-validation on the grid of your choice"
      ],
      "metadata": {
        "id": "3B-NJS3RRD-X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import linear_model, metrics, model_selection\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "grid_search = model_selection.GridSearchCV(\n",
        "    estimator=RandomForestClassifier(),\n",
        "    param_grid={'max_depth':[1, 3, 5, 10],\n",
        "                'criterion':['gini','entropy','log_loss']},\n",
        "    cv=10,\n",
        "    return_train_score=True)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "r = pd.DataFrame(grid_search.cv_results_)\n",
        "print(r[['params', 'mean_train_score', 'mean_test_score']])\n",
        "\n",
        "best_max_depth = grid_search.best_params_['max_depth']\n",
        "best_criterion = grid_search.best_params_['criterion']\n",
        "print('Best max_depth: %s' % best_max_depth)\n",
        "print('Best criterion: %s' % best_criterion)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iTWU2em9mAqZ",
        "outputId": "be72664b-5d1d-47c6-d36c-1f60b5b83b08"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                        params  mean_train_score  \\\n",
            "0        {'criterion': 'gini', 'max_depth': 1}          0.866807   \n",
            "1        {'criterion': 'gini', 'max_depth': 3}          0.978641   \n",
            "2        {'criterion': 'gini', 'max_depth': 5}          0.981586   \n",
            "3       {'criterion': 'gini', 'max_depth': 10}          0.999053   \n",
            "4     {'criterion': 'entropy', 'max_depth': 1}          0.881207   \n",
            "5     {'criterion': 'entropy', 'max_depth': 3}          0.979482   \n",
            "6     {'criterion': 'entropy', 'max_depth': 5}          0.980955   \n",
            "7    {'criterion': 'entropy', 'max_depth': 10}          0.999684   \n",
            "8    {'criterion': 'log_loss', 'max_depth': 1}          0.845438   \n",
            "9    {'criterion': 'log_loss', 'max_depth': 3}          0.979587   \n",
            "10   {'criterion': 'log_loss', 'max_depth': 5}          0.980744   \n",
            "11  {'criterion': 'log_loss', 'max_depth': 10}          0.999684   \n",
            "\n",
            "    mean_test_score  \n",
            "0          0.868221  \n",
            "1          0.977260  \n",
            "2          0.978203  \n",
            "3          0.978203  \n",
            "4          0.879748  \n",
            "5          0.977260  \n",
            "6          0.979155  \n",
            "7          0.978203  \n",
            "8          0.843693  \n",
            "9          0.978203  \n",
            "10         0.979155  \n",
            "11         0.978203  \n",
            "Best max_depth: 5\n",
            "Best criterion: entropy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "best_model = RandomForestClassifier(max_depth=best_max_depth, criterion=best_criterion)\n",
        "best_model.fit(X_train, y_train)\n",
        "y_predict_rf_model = best_model.predict(X_test)\n",
        "evaluate(best_model, X_train, y_train, X_test, y_test)"
      ],
      "metadata": {
        "id": "dQ1lbmIKRCR5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5f122e5-0c7c-45d7-e479-a048bc3dea13"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train accuracy: 0.9801136363636364\n",
            "Test accuracy: 0.9797794117647058\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Model Race: evaluate models (Logit, Logit + PCA, Classification forest) on the test dataset and choose the best one"
      ],
      "metadata": {
        "id": "8QbzL4viRkye"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# YOUR CODE HERE\n",
        "\n",
        "cm_logit = confusion_matrix(y_test, y_predict_logit_model)\n",
        "print(cm_logit)\n",
        "cm_logitpca = confusion_matrix(y_test, y_predict_logitpca_model)\n",
        "print(cm_logitpca)\n",
        "cm_rf = confusion_matrix(y_test, y_predict_rf_model)\n",
        "print(cm_rf)\n",
        "\n",
        "# All models result in same performance"
      ],
      "metadata": {
        "id": "w4lXcPWNRqn2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef23d94a-b139-4514-8b79-b59297872dce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[100   3   1   0]\n",
            " [  1 187   0   1]\n",
            " [  0   1 154   1]\n",
            " [  0   0   0  95]]\n",
            "[[100   3   1   0]\n",
            " [  1 187   0   1]\n",
            " [  0   1 154   1]\n",
            " [  0   0   0  95]]\n",
            "[[100   3   1   0]\n",
            " [  2 186   0   1]\n",
            " [  0   1 154   1]\n",
            " [  0   2   0  93]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "   \n",
        "\n",
        "\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "XL-b0dyFVpyN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![Alt Text](https://gifdb.com/images/high/gryffindor-sorting-hat-owbd80ftf54oai7i.webp)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "klSEvvXPTH6S"
      }
    }
  ]
}